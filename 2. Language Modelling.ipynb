{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/NER"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBt0F5HKEBmW","executionInfo":{"status":"ok","timestamp":1657907613433,"user_tz":240,"elapsed":25994,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}},"outputId":"f34bebea-6fe4-4f02-84cf-78fe59e4a0e8"},"id":"YBt0F5HKEBmW","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/NER\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/NER\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYR2ZOsu9xyf","executionInfo":{"status":"ok","timestamp":1657907621443,"user_tz":240,"elapsed":354,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}},"outputId":"8eb3eb1c-3065-46dc-817e-b5dd406636dd"},"id":"hYR2ZOsu9xyf","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["'1. LM Data Generation.ipynb'\t       '5 Final Entitiy Assign..ipynb'\n","'2.1 LM Pretrained Model'\t       'Copy of 2. Language Modelling.ipynb'\n","'2. Language Modelling.ipynb'\t        Final.csv\n","'3.1 Trained NER Model- 280-10 epoch'   predictions.pkl\n","'3. SCIBERT_TRAIN.ipynb'\t        pre_train_text.txt\n","'4. SCIBERT_INFER.ipynb'\t        ScopusJoournalsAll.xlsx\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiDq54oQD924","executionInfo":{"status":"ok","timestamp":1657907339290,"user_tz":240,"elapsed":11868,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}},"outputId":"0bf6a4ee-8ca3-444c-dc5f-8d198f3b07e5"},"id":"eiDq54oQD924","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.4 MB 32.5 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 68.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101 kB 12.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","execution_count":8,"id":"2ffb6990","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-07-05T10:14:10.549168Z","iopub.status.busy":"2022-07-05T10:14:10.548284Z","iopub.status.idle":"2022-07-05T10:14:18.118258Z","shell.execute_reply":"2022-07-05T10:14:18.117342Z"},"papermill":{"duration":7.577162,"end_time":"2022-07-05T10:14:18.120870","exception":false,"start_time":"2022-07-05T10:14:10.543708","status":"completed"},"tags":[],"id":"2ffb6990","executionInfo":{"status":"ok","timestamp":1657907685468,"user_tz":240,"elapsed":415,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}}},"outputs":[],"source":["from transformers import AutoModelWithLMHead, AutoTokenizer, AdamW, get_linear_schedule_with_warmup \n","from transformers import Trainer, TrainingArguments\n","from transformers import  DataCollatorForLanguageModeling, LineByLineTextDataset\n","from torch.utils.data import DataLoader, Dataset\n"]},{"cell_type":"code","execution_count":9,"id":"4273f1b6","metadata":{"execution":{"iopub.execute_input":"2022-07-05T10:14:18.127011Z","iopub.status.busy":"2022-07-05T10:14:18.125746Z","iopub.status.idle":"2022-07-05T10:14:18.131421Z","shell.execute_reply":"2022-07-05T10:14:18.130510Z"},"papermill":{"duration":0.010401,"end_time":"2022-07-05T10:14:18.133410","exception":false,"start_time":"2022-07-05T10:14:18.123009","status":"completed"},"tags":[],"id":"4273f1b6","executionInfo":{"status":"ok","timestamp":1657907688753,"user_tz":240,"elapsed":2,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}}},"outputs":[],"source":["class config:\n","    MODEL_PATH = \"allenai/scibert_scivocab_uncased\"\n","    TOKENIZER_PATH = \"allenai/scibert_scivocab_uncased\"\n","    SAVE_DIR = \"./\"\n","    BATCH_SIZE = 32\n","    EPOCHS = 25\n","    TOTAL_LEN  = 256\n","    SAVE_STEPS = 10000"]},{"cell_type":"code","execution_count":10,"id":"e8e6a9f2","metadata":{"execution":{"iopub.execute_input":"2022-07-05T10:14:18.137958Z","iopub.status.busy":"2022-07-05T10:14:18.137693Z","iopub.status.idle":"2022-07-05T10:15:48.538012Z","shell.execute_reply":"2022-07-05T10:15:48.533871Z"},"papermill":{"duration":90.405015,"end_time":"2022-07-05T10:15:48.540139","exception":false,"start_time":"2022-07-05T10:14:18.135124","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"e8e6a9f2","executionInfo":{"status":"ok","timestamp":1657907736600,"user_tz":240,"elapsed":43571,"user":{"displayName":"Abhijeet Bhattacharya","userId":"15036010312941972725"}},"outputId":"edf066b9-1438-410b-dd74-61489384e338"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n","Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["No. of lines:  52679\n"]}],"source":["model = AutoModelWithLMHead.from_pretrained(config.MODEL_PATH)\n","tokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH)\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")\n","\n","dataset= LineByLineTextDataset(\n","    tokenizer = tokenizer,  \n","    file_path = 'pre_train_text.txt',\n","    block_size = config.TOTAL_LEN  \n",")\n","print('No. of lines: ', len(dataset))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"595386fc","metadata":{"execution":{"iopub.execute_input":"2022-07-05T10:15:48.547588Z","iopub.status.busy":"2022-07-05T10:15:48.547272Z","iopub.status.idle":"2022-07-05T20:48:20.052071Z","shell.execute_reply":"2022-07-05T20:48:20.051182Z"},"papermill":{"duration":37951.511541,"end_time":"2022-07-05T20:48:20.054176","exception":false,"start_time":"2022-07-05T10:15:48.542635","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"595386fc","outputId":"ecda0202-a34a-479c-f047-c449a244a371"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 52679\n","  Num Epochs = 25\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 41175\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3767' max='41175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3767/41175 1:51:23 < 18:26:45, 0.56 it/s, Epoch 2.29/25]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.438500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.421800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.406100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.386600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.374400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.374600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.357100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["training_args = TrainingArguments(\n","    output_dir=config.SAVE_DIR,\n","    overwrite_output_dir=True,\n","    num_train_epochs=config.EPOCHS,\n","    per_device_train_batch_size=config.BATCH_SIZE,\n","    save_steps=config.SAVE_STEPS,\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n","\n",")\n","trainer.train()\n","trainer.save_model(config.SAVE_DIR)"]},{"cell_type":"code","source":[""],"metadata":{"id":"bums9MafEqbd"},"id":"bums9MafEqbd","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":38060.503023,"end_time":"2022-07-05T20:48:23.013158","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-07-05T10:14:02.510135","version":"2.3.4"},"colab":{"name":"Copy of 2. Language Modelling.ipynb","provenance":[{"file_id":"1z7kjDbIVLXTathiTi3aAkdYfMx7iKVdJ","timestamp":1657907250340}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}